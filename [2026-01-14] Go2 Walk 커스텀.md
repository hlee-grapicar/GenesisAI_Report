# Go2 Walk 예제 커스텀
여러 iteration 에 대해 go2 로봇이 어떻게 진화하여 학습되는지 6개  (iteration : 10, 100, 200, 300, 400, 500) 의 학습 결과를 go2 로봇 6 개의 개별 instance 에 적용해서 애니메이션하여 보이는 예제

## 변경 사항
### go2_train.py
<img width="816" height="54" alt="image" src="https://github.com/user-attachments/assets/74cd10fa-50d7-4e9a-aed6-2621b40cf81a" />

100번의 iteration 완료 시 pt 파일 저장에서 10번의 iteration 완료 시 pt 파일 저장으로 변경
<img width="786" height="47" alt="image" src="https://github.com/user-attachments/assets/2da6da78-a554-42b3-b79b-777d9458b738" />

iteration 최대 횟수를 501회로 변경

### go2_env.py
<img width="882" height="49" alt="image" src="https://github.com/user-attachments/assets/b56e03e2-aeff-4b24-bb38-977e50e82547" />
학습 결과 실행 시 여러 로봇 렌더링 위한 수정

<img width="958" height="337" alt="image" src="https://github.com/user-attachments/assets/0871f7ae-85fe-4601-b8cb-1e419cb8de8d" />

학습 결과 실행 시 화면에 로봇들이 보여질 때 offset을 적용하기 위한 분리

<img width="948" height="188" alt="image" src="https://github.com/user-attachments/assets/c007ba97-be73-4672-a8fd-a5aae1058208" />

여러 로봇들을 다루게 되었으므로 tensor의 차원을 확장

### go2_eval.py
<img width="749" height="52" alt="image" src="https://github.com/user-attachments/assets/437c0d53-af5d-4b5b-8857-2d20b0eaded1" />
<img width="925" height="53" alt="image" src="https://github.com/user-attachments/assets/c28eca56-51a7-460c-acaa-86b1e470ced0" />

--ckpt 인자로 다수의 체크포인트 파일을 전달받기 위한 수정

```
    # Load policies
    policies = []
    for ckpt in args.ckpt:
        # Create a new ActorCritic instance for each policy
        policy_cfg = train_cfg["policy"]
        policy = ActorCritic(
            num_actor_obs=env.num_obs,
            num_critic_obs=env.num_obs,
            num_actions=env.num_actions,
            actor_hidden_dims=policy_cfg["actor_hidden_dims"],
            critic_hidden_dims=policy_cfg["critic_hidden_dims"],
            activation=policy_cfg["activation"],
            init_noise_std=policy_cfg["init_noise_std"],
        )
        policy.to(gs.device)
        policy.eval()

        # Load the state dict from the checkpoint file
        resume_path = os.path.join(log_dir, f"model_{ckpt}.pt")
        print(f"Loading policy from: {resume_path}")
        loaded_dict = torch.load(resume_path, map_location=gs.device)
        policy.load_state_dict(loaded_dict["model_state_dict"])
        policies.append(policy)
```
동시에 다수의 robot을 개별적으로 통제하기 위해 OnPolicyRunner 방식에서 ActorCritic 방식으로 변경

## 시연 영상
- 좌측부터 500, 400, 300, 200, 100, 10 회 학습시킨 데이터
- 300회부터는 성능이 비슷하게 도출되는것으로 보임

https://github.com/user-attachments/assets/ceb2eeae-b442-4a7a-a6a7-5e4c9662c495


# 추가 사항 고찰
## 강화학습의 보상함수 구조
### Command(목표)
학습은 command_cfg에 설정한 값에 가까운 결과가 나올 수록 좋은 보상을 받게 됨
```
    command_cfg = {
        "num_commands": 3,
        "lin_vel_x_range": [0.5, 0.5],
        "lin_vel_y_range": [0, 0],
        "ang_vel_range": [0, 0],
    }
```
- lin_vel_x_range : x축 이동 속도(0.5 ~ 0.5)
- lin_vel_y_range : y축 이동 속도(0 ~ 0)
- ang_vel_range : 각속도(0 ~ 0)

### Reward(보상)
reward_cfg에 정의된 보상에 따라 계산된 보상을 받게 됨
```
    reward_cfg = {
        "tracking_sigma": 0.25,
        "base_height_target": 0.3,
        "feet_height_target": 0.075,
        "reward_scales": {
            "tracking_lin_vel": 1.0,
            "tracking_ang_vel": 0.2,
            "lin_vel_z": -1.0,
            "base_height": -50.0,
            "action_rate": -0.005,
            "similar_to_default": -0.1,
        },
    }
```
- trackin_sigma, base_height_target, feet_height_target : 보상함수 내에서 사용하는 민감도 또는 목표
- traking_lin_vel : 전후좌우 움직임, 가중치 1
- trackin_ang_vel : 각도 움직임, 가중치 2
- line_vel_z : 수직 움직임, 가중치 -1
- base_height : 몸통 높이, 가중치 -50
- action_rate : 이전 스텝과 현재 스텝의 명령값 차이, 가중치 -0.005
- similar_do_default : 기본 자세와 유사성 : 가중치 -0.1

### 보상함수
```
_reward_tracking_lin_vel(self)

R_{lin\_vel,i}: i번째 로봇의 선형 속도 추종 보상
v_{cmd,i}: i번째 로봇의 명령된 선형 속도 벡터 (x, y)
v_{base,i}: i번째 로봇의 실제 선형 속도 벡터 (x, y)
σ_track: tracking_sigma 파라미터

오차 계산
E_i = (v_{cmd,ix} - v_{base,ix})^2 + (v_{cmd,iy} - v_{base,iy})^2
보상 계산
R_{lin_vel,i} = e^(-E_i / σ_track)
```
목표 속도와 실제 속도의 차이. 지수함수를 사용하기에 차이가 클수록 0에 가까운 보상을 받음

```
_reward_tracking_ang_vel(self)

R_{ang\_vel,i}: i번째 로봇의 각속도 추종 보상
w_{cmd,z,i}: i번째 로봇의 명령된 Z축 각속도
w_{base,z,i}: i번째 로봇의 실제 Z축 각속도
σ_track: tracking_sigma 파라미터

오차 계산 
E_{ang,i} = (w_{cmd,z,i} - w_{base,z,i})^2
보상 계산
R_{ang\_vel,i} = e^(-E_{ang,i} / σ_track)
```
목표 각속도와 실제 각속도의 차이. 지수함수를 사용하기에 차이가 클수록 0에 가까운 보상을 받음

```
_reward_lin_vel_z(self):

R_{action\_rate,i}: i번째 로봇의 행동 변화율 벌점
a_{t,i}: i번째 로봇의 현재 행동 벡터 (12개 관절)
a_{t-1,i}: i번째 로봇의 이전 시간 스텝 행동 벡터 (12개 관절)
K: 행동의 개수 (관절의 개수, 여기서는 12)

보상 계산
R_{action\_rate,i} = Σ_{j=1}^{K} (a_{t-1,j,i} - a_{t,j,i})^2
```
수직 속도 계산
```
_reward_action_rate(self)
R_{action\_rate,i}: i번째 로봇의 행동 변화율 벌점
a_{t,i}: i번째 로봇의 현재 행동 벡터 (12개 관절)
a_{t-1,i}: i번째 로봇의 이전 시간 스텝 행동 벡터 (12개 관절)
K: 행동의 개수 (관절의 개수, 여기서는 12)

보상 계산
R_{action\_rate,i} = Σ_{j=1}^{K} (a_{t-1,j,i} - a_{t,j,i})^2
```
이전 액션과 현재 액션의 차이 계산
```
_reward_similar_to_default(self)

R_{default\_sim,i}: i번째 로봇의 기본 자세 유사성 벌점
q_{curr,i}: i번째 로봇의 현재 관절 위치 벡터 (12개 관절)
q_{def,i}: i번째 로봇의 기본 자세 관절 위치 벡터 (12개 관절)
K: 관절의 개수 (여기서는 12)

보상 계산
R_{default\_sim,i} = Σ_{j=1}^{K} |q_{curr,j,i} - q_{def,j,i}|
```
기본 자세와의 차이 계산
```
_reward_base_height(self)

R_{base\_height,i}: i번째 로봇의 몸통 높이 벌점
h_{curr,z,i}: i번째 로봇 몸통의 현재 Z축 위치 (높이)
h_{target,z}: 목표 몸통 높이

보상 계산
R_{base\_height,i} = (h_{curr,z,i} - h_{target,z})^2
```
지면과 몸통 높이 계산

## 학습 모델의 MLP 구조
```
  +----------------------------------------------------------------------------------+
    |                              ActorCritic Model                                   |
    | (관측값을 입력받아 행동을 결정하고, 상태 가치를 평가하는 복합 모델)                  |
    +----------------------------------------------------------------------------------+
                                            |
                                            | (입력: 로봇의 현재 관측 상태)
                                            |
                                            v
                                +--------------------------+
                                |     입력층 (Input Layer)   |
                                |       (뉴런 수: 45개)      |
                                +--------------------------+
                                            |
                     +----------------------+----------------------+
                     |                                             |
                     v                                             v
            +--------------------------+                  +--------------------------+
            |        Actor MLP         |                  |        Critic MLP        |
            |      (정책 신경망)       |                  |       (가치 신경망)      |
            +--------------------------+                  +--------------------------+
            |                          |                  |                          |
            | +----------------------+ |                  | +----------------------+ |
            | |  은닉층 1 (Hidden 1) | |                  | |  은닉층 1 (Hidden 1) | |
            | |    (뉴런 수: 512개)   | |                  | |    (뉴런 수: 512개)   | |
            | |  활성화 함수: ELU   | |                  | |  활성화 함수: ELU   | |
            | +----------------------+ |                  | +----------------------+ |
            |          |                 |                  |          |                 |
            |          v                 |                  |          v                 |
            | +----------------------+ |                  | +----------------------+ |
            | |  은닉층 2 (Hidden 2) | |                  | |  은닉층 2 (Hidden 2) | |
            | |    (뉴런 수: 256개)   | |                  | |    (뉴런 수: 256개)   | |
            | |  활성화 함수: ELU   | |                  | |  활성화 함수: ELU   | |
            | +----------------------+ |                  | +----------------------+ |
            |          |                 |                  |          |                 |
            |          v                 |                  |          v                 |
            | +----------------------+ |                  | +----------------------+ |
            | |  은닉층 3 (Hidden 3) | |                  | |  은닉층 3 (Hidden 3) | |
            | |    (뉴런 수: 128개)   | |                  | |    (뉴런 수: 128개)   | |
            | |  활성화 함수: ELU   | |                  | |  활성화 함수: ELU   | |
            | +----------------------+ |                  | +----------------------+ |
            |          |                 |                  |          |                 |
            |          v                 |                  |          v                 |
            | +----------------------+ |                  | +----------------------+ |
            | |   출력층 (Output)    | |                  | |   출력층 (Output)    | |
            | |    (뉴런 수: 12개)    | |                  | |    (뉴런 수: 1개)     | |
            | |    (로봇 행동 값)    | |                  | |    (상태 가치 값)    | |
            | +----------------------+ |                  | +----------------------+ |
            +--------------------------+                  +--------------------------+
```
- "class_name": "ActorCritic" -> ActroCritic 모델 사용
- "actor_hidden_dims": [512, 256, 128], "critic_hidden_dims": [512, 256, 128] -> 정책 신경망과 가치 신경망 각각 512, 256, 128개 뉴련의 은닉층 3개 사용
- "activation": "elu" -> ELU 활성화 함수 사용

- 정책 신경망은 로봇의 12개 관절에 대한 목표 행동 값 출력
- 가치 신경망은 현재 로봇 상태의 가치 출력
- 가치 신경망이 예측한 값보다 보상이 크면 좋은 행동이라 판단하여 강화, 작으면 나쁜 행동이라 판단하여 억제

- 은닉 신경망의 수와 뉴련의 수는 너무 많으면 overfitting, 너무 적으면 underfitting이 일어남. 경험적 판단에 의한 적정 수 설정 필요.

## MLP 의 학습 파라미터 수
### Actor MLP (정책 신경망)
   - 입력(45) → 은닉층1(512): (45 * 512) + 512 = 23,552개
   - 은닉층1(512) → 은닉층2(256): (512 * 256) + 256 = 131,328개
   - 은닉층2(256) → 은닉층3(128): (256 * 128) + 128 = 32,896개
   - 은닉층3(128) → 출력(12): (128 * 12) + 12 = 1,548개

  - Actor MLP 총 파라미터 수: 189,324개
### Critic MLP (가치 신경망)
   - 입력(45) → 은닉층1(512): (45 * 512) + 512 = 23,552개
   - 은닉층1(512) → 은닉층2(256): (512 * 256) + 256 = 131,328개
   - 은닉층2(256) → 은닉층3(128): (256 * 128) + 128 = 32,896개
   - 은닉층3(128) → 출력(1): (128 * 1) + 1 = 129개

  - Critic MLP 총 파라미터 수: 187,905개

### 총 학습 파라미터 수
- 189,324 (Actor) + 187,905 (Critic) = 377,229개
  ---
- 계산 시 곱셈 외에 덧셈을 진행하는 이유 : 편향을 주기 위해서
- 가중치만으로 표현할 수 없는 유연성을 모델에 부여

## 학습 소요 시간
- max_iterations = 501 일 때 대략 7700초 소요(약 2시간 8분)
- GPU 환경에서 학습 시 보다 빠른 결과를 얻을 것으로 추정

## 학습 시 score/loss 변화는 어떻게 tracking 되는가
- runner의 "log_interval": 1, 설정으로 한번의 iteration이 완료 될 때마다 콘솔에 출력됨
```
################################################################################
                        Learning iteration 1/501 

                       Computation: 8756 steps/s (collection: 3.185s, learning 8.041s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0058
             Mean action noise std: 1.00
                 Mean total reward: -0.17
               Mean episode length: 27.17
 Mean episode rew_tracking_lin_vel: 0.0060
 Mean episode rew_tracking_ang_vel: 0.0033
        Mean episode rew_lin_vel_z: -0.0067
      Mean episode rew_base_height: -0.0041
      Mean episode rew_action_rate: -0.0028
Mean episode rew_similar_to_default: -0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 11.23s
                        Total time: 26.84s
                               ETA: 6710.7s
```
- Value function loss: 가치 함수 손실. 낮을수록 Critic의 예측이 정확하다는 것을 의미
- Mean total reward: 이번 iteration 동안 완료된 에피소드들에서 로봇이 얻은 평균 총점
