# Go2 Walk 예제 커스텀
여러 iteration 에 대해 go2 로봇이 어떻게 진화하여 학습되는지 6개  (iteration : 10, 100, 200, 300, 400, 500) 의 학습 결과를 go2 로봇 6 개의 개별 instance 에 적용해서 애니메이션하여 보이는 예제

## 변경 사항
### go2_train.py
<img width="816" height="54" alt="image" src="https://github.com/user-attachments/assets/74cd10fa-50d7-4e9a-aed6-2621b40cf81a" />

100번의 iteration 완료 시 pt 파일 저장에서 10번의 iteration 완료 시 pt 파일 저장으로 변경
<img width="786" height="47" alt="image" src="https://github.com/user-attachments/assets/2da6da78-a554-42b3-b79b-777d9458b738" />

iteration 최대 횟수를 501회로 변경

### go2_env.py
<img width="882" height="49" alt="image" src="https://github.com/user-attachments/assets/b56e03e2-aeff-4b24-bb38-977e50e82547" />
학습 결과 실행 시 여러 로봇 렌더링 위한 수정

<img width="958" height="337" alt="image" src="https://github.com/user-attachments/assets/0871f7ae-85fe-4601-b8cb-1e419cb8de8d" />

학습 결과 실행 시 화면에 로봇들이 보여질 때 offset을 적용하기 위한 분리

<img width="948" height="188" alt="image" src="https://github.com/user-attachments/assets/c007ba97-be73-4672-a8fd-a5aae1058208" />

여러 로봇들을 다루게 되었으므로 tensor의 차원을 확장

### go2_eval.py
<img width="749" height="52" alt="image" src="https://github.com/user-attachments/assets/437c0d53-af5d-4b5b-8857-2d20b0eaded1" />
<img width="925" height="53" alt="image" src="https://github.com/user-attachments/assets/c28eca56-51a7-460c-acaa-86b1e470ced0" />

--ckpt 인자로 다수의 체크포인트 파일을 전달받기 위한 수정

```
    # Load policies
    policies = []
    for ckpt in args.ckpt:
        # Create a new ActorCritic instance for each policy
        policy_cfg = train_cfg["policy"]
        policy = ActorCritic(
            num_actor_obs=env.num_obs,
            num_critic_obs=env.num_obs,
            num_actions=env.num_actions,
            actor_hidden_dims=policy_cfg["actor_hidden_dims"],
            critic_hidden_dims=policy_cfg["critic_hidden_dims"],
            activation=policy_cfg["activation"],
            init_noise_std=policy_cfg["init_noise_std"],
        )
        policy.to(gs.device)
        policy.eval()

        # Load the state dict from the checkpoint file
        resume_path = os.path.join(log_dir, f"model_{ckpt}.pt")
        print(f"Loading policy from: {resume_path}")
        loaded_dict = torch.load(resume_path, map_location=gs.device)
        policy.load_state_dict(loaded_dict["model_state_dict"])
        policies.append(policy)
```
동시에 다수의 robot을 개별적으로 통제하기 위해 OnPolicyRunner 방식에서 ActorCritic 방식으로 변경

## 시연 영상
- 좌측부터 500, 400, 300, 200, 100, 10 회 학습시킨 데이터
- 300회부터는 성능이 비슷하게 도출되는것으로 보임

https://github.com/user-attachments/assets/ceb2eeae-b442-4a7a-a6a7-5e4c9662c495

