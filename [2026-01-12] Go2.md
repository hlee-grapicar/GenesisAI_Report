# Go2

## 목적
개 모양 로봇을 강화학습을 통해 걸어가도록 학습 시키는 것
go2_train에서 runner를 이용해 go2_env의 환경 및 시뮬레이션을 진행하여 학습, 학습된 결과를 go2_eval에서 실행
특정 목표를 향해 학습하는 것이 아닌 보상에 따른 상태 학습

## 파일 분석
# go2_env.py
- 환경 정의 및 시뮬레이터 연결 : genesis 물리 시뮬레이터와 강화학습 프레임워크를 연결하여 로봇의 행동을 시뮬레이션하고, 상태 측정, 보상 계산, 환경 초기화 등을 담당.
- __init__() : 인자로 받은 값들을 통해 시뮬레이션에 필요한 초기화 진행
- command :  환경이 로봇에게 내리는 순간적인 목표 지시. command를 따르면 보상을 받음
- action : 정책 신경망이 command를 달성하기 위해 어떻게 움직여야 할지에 대한 명령을 내린 값
- observation : 현재 로봇의 상태

- 보상 값 계산하는 reward 함수들로 action에 대한 보상 지급
 ```
  def _reward_tracking_lin_vel(self):
        # Tracking of linear velocity commands (xy axes)
        lin_vel_error = torch.sum(torch.square(self.commands[:, :2] - self.base_lin_vel[:, :2]), dim=1)
        return torch.exp(-lin_vel_error / self.reward_cfg["tracking_sigma"])

    def _reward_tracking_ang_vel(self):
        # Tracking of angular velocity commands (yaw)
        ang_vel_error = torch.square(self.commands[:, 2] - self.base_ang_vel[:, 2])
        return torch.exp(-ang_vel_error / self.reward_cfg["tracking_sigma"])

    def _reward_lin_vel_z(self):
        # Penalize z axis base linear velocity
        return torch.square(self.base_lin_vel[:, 2])

etc ...
```
- observation으로 로봇의 상태에 대한 정보를 제공받은 policy가 정한 action 값을 이용하여 runner가 env의 step을 실행. 실행 결과에 따라 reward 계산 후 반영
- ```
          # compute reward
        self.rew_buf.zero_()
        for name, reward_func in self.reward_functions.items():
            rew = reward_func() * self.reward_scales[name]
            self.rew_buf += rew
            self.episode_sums[name] += rew
  ```
- 과적합 방지를 위하여 일정 step이 진행된 후에는 command를 resample

  ---

# go2_train.py
- 실제 훈련에 사용할 정보들을 제공 및 훈련
- train_cfg_dict : 훈련에 필요한 설정들 (훈련 알고리즘, 반복 횟수 등)
- env_cfg : 완경 정보들
- obs_cfg : 관측시 사용할 항목들의 scale 값 지정
- reward_cfg : 보상 시 필요한 값들 및 보상에 가중치를 지정
- command_cfg : command의 특성을 설정
- pickle을 통하여 cfg 저장
- OnPolicyRunner를 사용하여 학습 진행, 학습 결과는 runnder가 model_n.pt로 제공해줌

---
# go2_eval.py
- 훈련한 데이터를 이용해 로봇을 행동하도록 명령
- 저장한 pickle 파일을 이용해 env cfg들을 load(훈련 환경과 같은 환경을 만들기 위함)
- OnPolicyRunner를 통해 model_n.pt를 load
- step에 따라 변화되는 obs를 보고 runner가 model_n.pt에 따라 판단하여 로봇 실행
- ```
      with torch.no_grad():
        while True:
            actions = policy(obs)
            obs, rews, dones, infos = env.step(actions)
  ```


https://github.com/user-attachments/assets/109c8da7-d024-4820-acda-139bdc28670f

---

# go2_backflip.py
- 이미 학습된 .pt 데이터를 가지고 실행하는 예제
- env값을 직접 설정해줌과 동시에 observation 항목을 추가해줌(backflip에 필요한 값)

https://github.com/user-attachments/assets/8b5171d8-b68f-4912-99ea-ac9446d32832


  
